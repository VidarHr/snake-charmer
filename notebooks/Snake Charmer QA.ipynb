{
 "metadata": {
  "name": "",
  "signature": "sha256:ed50b28d3e1becd666a9086cd0aabf5d44faf46d77c0a8d828b78149d49fd43a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Snake Charmer Quality Assurance Dept.\n",
      "## _\"All Your Eggs In One Basket\"_\n",
      "\n",
      "Welcome to the Quality Assurance Department. Here you can run tests against Snake Charmer's components, in parallel, using as many workers as you have vCPUs assigned to the VM.\n",
      "\n",
      "The results will be written to `log/<hostname>/test_output` (relative to your `snake-charmer` directory).\n",
      "\n",
      "**Warning:** Running the whole suite will take quite some time, even with lots of vCPUs, as some of the components' test suites take well over an hour each."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Initialization and helper functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import subprocess\n",
      "import functools\n",
      "from concurrent import futures\n",
      "import os\n",
      "import shutil\n",
      "import math\n",
      "from itertools import *\n",
      "import sys\n",
      "import datetime\n",
      "import re\n",
      "\n",
      "# TODO we'll need to use this for earlier versions of Python: https://pypi.python.org/pypi/futures\n",
      "\n",
      "cpus = os.cpu_count()\n",
      "workers = cpus\n",
      "\n",
      "outdir = '/srv/log/test_output'\n",
      "python = 'python3.4' # FIXME get this automatically\n",
      "\n",
      "shutil.rmtree(outdir, ignore_errors=True)\n",
      "os.makedirs(outdir, exist_ok=True)\n",
      "\n",
      "def log(string, f):\n",
      "    print(string, flush=True)\n",
      "    print(string, flush=True, file=f)\n",
      "\n",
      "def get_immediate_subdirectories(dir):\n",
      "    return [name for name in os.listdir(dir)\n",
      "            if os.path.isdir(os.path.join(dir, name))]\n",
      "\n",
      "def make_cmd_line(name, command):\n",
      "    return '%s > %s/%s.out 2>&1' % (command.replace('$PYTHON', python), outdir, name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Build lists of test commands and execution functions\n",
      "\n",
      "#### TODO Simple timestamping of output starts/ends/duration\n",
      "#### TODO Use logs instead of print"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "serial_test_commands = [\n",
      "    ('ipython', '$PYTHON `which iptest3` --all -j'),\n",
      "    ('joblib', '$PYTHON `which nosetests` joblib'),\n",
      "    ('numba', '$PYTHON -c \"import numba; numba.testing.multitest()\"')\n",
      "]\n",
      "\n",
      "# Generate individual submodule tests for sympy, pytables and theano, as they take forever if run in one process\n",
      "\n",
      "def make_sympy_test_cmd(submod):\n",
      "    name = 'sympy_%s' % submod\n",
      "    cmd = '$PYTHON -c \"import sympy; sympy.test(\\'/%s\\')\"' % submod\n",
      "    return (name, cmd)\n",
      "\n",
      "sympy_test_commands = (make_sympy_test_cmd(submod)\n",
      "                       for submod in get_immediate_subdirectories('/usr/local/lib/%s/dist-packages/sympy' % python)\n",
      "                       if not submod.startswith('_'))\n",
      "\n",
      "name_re = re.compile('theano.*')\n",
      "def make_theano_test_cmd(script, test_dir):\n",
      "    path = '%s/%s' % (test_dir, script)\n",
      "    match = name_re.search(path)\n",
      "    if match is None:\n",
      "        raise(RuntimeError('regex failed on string %s' % path))\n",
      "    name = match.group(0).replace('/', '_').replace('.py', '')\n",
      "    test_cache = '/tmp/' + name\n",
      "    cmd = 'rm -rf %s && THEANO_FLAGS=\"compiledir=%s\" $PYTHON `which theano-nose` %s' % (test_cache, test_cache, path)\n",
      "    return (name, cmd)\n",
      "\n",
      "theano_dir = '/usr/local/lib/%s/dist-packages/theano' % python\n",
      "theano_test_commands = [make_theano_test_cmd(script, location[0])\n",
      "                        for location in os.walk(theano_dir)\n",
      "                        for script in location[2]\n",
      "                        if script.startswith('test_') and script.endswith('.py')]\n",
      "\n",
      "def make_pytables_test_cmd(script, test_dir):\n",
      "    name = script.replace('test_', 'pytables_').replace('.py', '')\n",
      "    cmd = '$PYTHON %s/%s --heavy' % (test_dir, script)\n",
      "    return (name, cmd)\n",
      "\n",
      "pytables_test_dir = '/usr/local/lib/%s/dist-packages/tables/tests' % python\n",
      "pytables_test_commands = [make_pytables_test_cmd(script, pytables_test_dir)\n",
      "                          for script in os.listdir(pytables_test_dir)\n",
      "                          if script.startswith('test_') and script.endswith('.py') and script != 'test_all.py']\n",
      "\n",
      "# Make sure test_queries.py is at the start, as it's a monster!\n",
      "\n",
      "def pred(elem):\n",
      "    return elem[0] != 'pytables_queries'\n",
      "\n",
      "pytables_test_commands_reordered = chain(dropwhile(pred, pytables_test_commands),\n",
      "                                         takewhile(pred, pytables_test_commands))\n",
      "\n",
      "misc_test_commands = [\n",
      "    ('cython', '(cd /srv/cache/src/Cython/ && $PYTHON runtests.py -vv)'),\n",
      "    ('pymc', '$PYTHON -c \"import pymc; pymc.test(\\'full\\')\"'),\n",
      "    ('matplotlib', '$PYTHON -c \"import matplotlib; matplotlib.test()\"'),\n",
      "    ('numpy', '$PYTHON -c \"import numpy; numpy.test(\\'full\\')\"'),\n",
      "    ('scipy', '$PYTHON -c \"import scipy; scipy.test(\\'full\\')\"'),\n",
      "    ('pandas', '$PYTHON `which nosetests` pandas'),\n",
      "    ('sklearn', '$PYTHON `which nosetests` sklearn --exe'),\n",
      "    ('pillow', '(cd /tmp && rm -f Images && ln -s /srv/cache/src/Pillow/Images . && $PYTHON /srv/cache/src/Pillow/selftest.py --installed)'),\n",
      "    ('nltk', '$PYTHON /usr/local/lib/$PYTHON/dist-packages/nltk/test/runtests.py'),\n",
      "    ('gensim', '(cd /srv/cache/src/gensim && $PYTHON setup.py test)'),\n",
      "    ('bottleneck', '$PYTHON -c \"import bottleneck as bn; bn.test(\\'full\\')\"'),\n",
      "    ('statsmodels', '$PYTHON -c \"import matplotlib as mpl; mpl.use(\\'Agg\\'); import statsmodels as sm; sm.test()\"'),\n",
      "    ('numexpr', '$PYTHON `which nosetests` numexpr'),\n",
      "    ('patsy', '$PYTHON `which nosetests` patsy'),\n",
      "    ('prettyplotlib', '(cd /srv/cache/src/prettyplotlib/tests && for py in $(ls test_*.py); do $PYTHON $py; done)')\n",
      "]\n",
      "\n",
      "# Interleave these four command lists, to (attempt to) get good balance of CPU and IO heavy tasks\n",
      "\n",
      "parallel_test_commands = list(filter(None,\n",
      "                                     chain(*zip_longest(pytables_test_commands_reordered,\n",
      "                                                        theano_test_commands,\n",
      "                                                        misc_test_commands,\n",
      "                                                        sympy_test_commands))))\n",
      "\n",
      "def run_serial_tests(*args, logfile):\n",
      "    if len(args) > 0:\n",
      "        names = set(args)\n",
      "        num = len(names)\n",
      "    else:\n",
      "        names = None\n",
      "        num = len(serial_test_commands)\n",
      "    log('Running %d tests\\n' % num, logfile)\n",
      "    for name, command in serial_test_commands:\n",
      "        if not names or name in names:\n",
      "            cmd_line = make_cmd_line(name, command)\n",
      "            log('%s: %s\\n' % (name, cmd_line), logfile)\n",
      "            rc = subprocess.call(cmd_line, shell=True, executable='/bin/bash')\n",
      "            if rc == 0:\n",
      "                log('%s: Completed\\n' % name, logfile)\n",
      "            else:\n",
      "                log('%s: **** FAILED! Exited with status %d\\n' % (name, rc), logfile)\n",
      "\n",
      "def run_parallel_tests(*args, logfile):\n",
      "    names = set(args) if len(args) > 0 else None\n",
      "    with futures.ThreadPoolExecutor(max_workers=workers) as pool:\n",
      "        log('Submitting tasks to pool of %d workers\\n' % workers, logfile)\n",
      "        cmd_lines = [(name, make_cmd_line(name, command))\n",
      "                     for name, command in parallel_test_commands]\n",
      "        \n",
      "        task_data = []\n",
      "        for name, cmd_line in cmd_lines:\n",
      "            if not names or name in names:\n",
      "                log('Submitting %s: %s\\n' % (name, cmd_line), logfile)\n",
      "                task_data.append((pool.submit(subprocess.call,\n",
      "                                     cmd_line,\n",
      "                                     shell=True,\n",
      "                                     executable='/bin/bash'),\n",
      "                                  (name, cmd_line)))\n",
      "        futures_map = dict(task_data)\n",
      "        expected = len(futures_map)\n",
      "\n",
      "        completed = 0\n",
      "        for future in futures.as_completed(futures_map):\n",
      "            completed += 1\n",
      "            name = futures_map[future][0]\n",
      "            if future.exception() is not None:\n",
      "                log('%s (%d of %d): **** FAILED!\\n\\n%s\\n' %\n",
      "                      (name, completed, expected, repr(future.exception())),\n",
      "                      logfile)\n",
      "            elif future.result() != 0:\n",
      "                log('%s (%d of %d): **** FAILED! Exited with status %d\\n' %\n",
      "                      (name, completed, expected, future.result()),\n",
      "                      logfile)\n",
      "            else:\n",
      "                log('%s (%d of %d): Completed\\n' %\n",
      "                      (name, completed, expected),\n",
      "                      logfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Run serial tests\n",
      "\n",
      "These are tests to run in serial, as they handle their own parallelism internally. Each one will use as many vCPUs as it can get its hands on, so we let each one monopolize the VM until it's finished.\n",
      "\n",
      "When tests are started or completed, status messages appear below the next cell. If a test causes an exception or returns a non-zero exit code, check the corresponding output file in `log/<hostname>/test_output`. A summary of the test run will appear in `log/<hostname>/serial.log`.\n",
      "\n",
      "You can optionally provide one or more test names, to execute those tests only."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(outdir + '/serial.log', 'w') as f:\n",
      "    run_serial_tests(logfile=f)\n",
      "# Optionally, give a subset to run, e.g.:\n",
      "# run_serial_tests('joblib', 'ipython', logfile=f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Parallel tests\n",
      "\n",
      "These are tests to run in parallel explicitly, after the serial tests have finished. Each one runs in a separate isolated Python process. They are coordinated by a thread pool within the parent process, so that no more than `workers` processes are running at once, but in some cases a test may use more than a whole vCPU, because of starting helper threads or subprocesses.\n",
      "\n",
      "In general we try to start the slower-running tests earlier.\n",
      "\n",
      "When jobs are submitted or completed, status messages appear below the next cell. If a test causes an exception or returns a non-zero exit code, check the corresponding output file in `log/<hostname>/test_output`. A summary of the test run will appear in `log/<hostname>/parallel.log`.\n",
      "\n",
      "You can optionally provide one or more test names, to execute those tests only."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(outdir + '/parallel.log', 'w') as f:\n",
      "    run_parallel_tests(logfile=f)\n",
      "# Optionally, give a subset to run, e.g.:\n",
      "# run_parallel_tests('pillow', 'sympy_physics', 'pytables_links', logfile=f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}